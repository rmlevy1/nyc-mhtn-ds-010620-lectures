{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.771300\n",
      "F1: 0.666667\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.07242723, 0.19350638, 0.31015968, 0.06849694, 0.18681998,\n",
       "        0.30772452, 0.06573558, 0.18770051, 0.31062937, 0.08435583,\n",
       "        0.21679916, 0.36321554, 0.07697845, 0.21611171, 0.35266647,\n",
       "        0.07778955, 0.21241798, 0.34899082, 0.08464694, 0.24526238,\n",
       "        0.3970891 , 0.08461094, 0.24058642, 0.39614921, 0.08971744,\n",
       "        0.2380343 , 0.38331804, 0.09398866, 0.26410913, 0.44260831,\n",
       "        0.09568944, 0.26675344, 0.43079967, 0.092945  , 0.2579473 ,\n",
       "        0.41623392, 0.10241618, 0.28912387, 0.47500825, 0.10676641,\n",
       "        0.28299918, 0.45434875, 0.09935198, 0.27127166, 0.43158064,\n",
       "        0.06663699, 0.18630533, 0.30564823, 0.06685648, 0.18658395,\n",
       "        0.30479188, 0.06516285, 0.19657598, 0.33453922, 0.08430157,\n",
       "        0.25341997, 0.36348577, 0.07842832, 0.21742687, 0.38399644,\n",
       "        0.08643951, 0.22575288, 0.36104531, 0.09011593, 0.2575418 ,\n",
       "        0.41022081, 0.08511753, 0.23815603, 0.39919682, 0.08307977,\n",
       "        0.24056578, 0.39222455, 0.0937654 , 0.27045684, 0.44259653,\n",
       "        0.09437189, 0.26284094, 0.42827692, 0.0896874 , 0.25218987,\n",
       "        0.41861787, 0.10124054, 0.29127369, 0.50211997, 0.10078692,\n",
       "        0.28074007, 0.45391808, 0.09748516, 0.27126379, 0.48235054,\n",
       "        0.09027357, 0.22423325, 0.3104825 , 0.06521239, 0.19172835,\n",
       "        0.30852499, 0.0677402 , 0.33501897, 0.42117624, 0.08732252,\n",
       "        0.24697037, 0.37934527, 0.08321204, 0.21760273, 0.35938001,\n",
       "        0.07732821, 0.22480974, 0.35772457, 0.0846025 , 0.24486575,\n",
       "        0.40478964, 0.08431144, 0.24399395, 0.39886088, 0.08491244,\n",
       "        0.23409634, 0.39892998, 0.09481759, 0.27205796, 0.46102986,\n",
       "        0.09738498, 0.28095922, 0.44653578, 0.09485917, 0.26400223,\n",
       "        0.44209776, 0.10832658, 0.30443535, 0.49928288, 0.10651641,\n",
       "        0.29558315, 0.59452128, 0.14149446, 0.40162401, 0.49628878,\n",
       "        0.07429004, 0.20569186, 0.32416215, 0.07056475, 0.19625959,\n",
       "        0.32484732, 0.06975422, 0.19711533, 0.32759581, 0.08060632,\n",
       "        0.23431497, 0.37278671, 0.07983718, 0.22757583, 0.37427697,\n",
       "        0.08303404, 0.2313282 , 0.37948503, 0.09571371, 0.26578088,\n",
       "        0.49023848, 0.1018322 , 0.40433416, 0.45312376, 0.09559216,\n",
       "        0.32701435, 0.55728045, 0.1267828 , 0.30415497, 0.49049783,\n",
       "        0.09703431, 0.2790689 , 0.4617919 , 0.10085979, 0.27550459,\n",
       "        0.45091481, 0.10594535, 0.34475603, 0.50942225, 0.10493631,\n",
       "        0.30774722, 0.51128836, 0.10446839, 0.29960661, 0.50278559,\n",
       "        0.07605705, 0.20972247, 0.35097737, 0.07686696, 0.21406417,\n",
       "        0.34545732, 0.07994609, 0.20874224, 0.34054375, 0.08577332,\n",
       "        0.24148817, 0.39997635, 0.08711505, 0.23778634, 0.39614573,\n",
       "        0.08506961, 0.23448458, 0.39602895, 0.09611502, 0.27570276,\n",
       "        0.45720582, 0.09613028, 0.27413359, 0.46298699, 0.09538832,\n",
       "        0.29004726, 0.4860352 , 0.11625705, 0.30970087, 0.51234369,\n",
       "        0.10912805, 0.30947948, 0.50117126, 0.10570416, 0.30420961,\n",
       "        0.49528217, 0.12196207, 0.33267212, 0.55417976, 0.11009159,\n",
       "        0.32182841, 0.53582916, 0.11467204, 0.38412409, 0.6023828 ,\n",
       "        0.08283629, 0.21785407, 0.3401638 , 0.07555523, 0.20836282,\n",
       "        0.34364491, 0.07880383, 0.22422423, 0.34897523, 0.08315949,\n",
       "        0.23595176, 0.39689598, 0.08549652, 0.24142017, 0.39574933,\n",
       "        0.08913727, 0.25042839, 0.40991216, 0.09561086, 0.27112093,\n",
       "        0.45417633, 0.09662466, 0.27005463, 0.44457703, 0.09933124,\n",
       "        0.28224425, 0.44473195, 0.10582161, 0.52248979, 0.5494184 ,\n",
       "        0.1022182 , 0.2808104 , 0.61857057, 0.10493851, 0.31683483,\n",
       "        0.47055931, 0.11646042, 0.33443532, 0.52917366, 0.11368346,\n",
       "        0.30477414, 0.51338491, 0.10528073, 0.31704354, 0.64731603,\n",
       "        0.07710485, 0.22348595, 0.38904514, 0.11020513, 0.26713867,\n",
       "        0.34615088, 0.07946568, 0.23186016, 0.3641921 , 0.08403316,\n",
       "        0.24618354, 0.39733415, 0.08719726, 0.24418001, 0.40646234,\n",
       "        0.0839921 , 0.23742957, 0.39115996, 0.10105972, 0.2756711 ,\n",
       "        0.44833398, 0.09637504, 0.27262859, 0.44686995, 0.09740257,\n",
       "        0.26394372, 0.42475286, 0.10266862, 0.28795924, 0.47078524,\n",
       "        0.10047641, 0.29293561, 0.47812414, 0.10442853, 0.28461852,\n",
       "        0.46309042, 0.11458182, 0.31856637, 0.5199522 , 0.1041976 ,\n",
       "        0.30303698, 0.50580955, 0.10687861, 0.29190612, 0.48816118,\n",
       "        0.07669554, 0.21129365, 0.35260286, 0.07656837, 0.2184495 ,\n",
       "        0.34937458, 0.08094101, 0.22135758, 0.35290375, 0.08607426,\n",
       "        0.23602309, 0.40041351, 0.08643579, 0.2343544 , 0.3855998 ,\n",
       "        0.08066359, 0.24053988, 0.39188762, 0.09769888, 0.27616963,\n",
       "        0.4509234 , 0.09606252, 0.26477556, 0.4205812 , 0.08920183,\n",
       "        0.2549684 , 0.42413464, 0.10100579, 0.2955163 , 0.4904633 ,\n",
       "        0.11030188, 0.29266124, 0.48226624, 0.10479374, 0.29164281,\n",
       "        0.46514893, 0.10859342, 0.30824251, 0.52048326, 0.10900059,\n",
       "        0.30253139, 0.4913599 , 0.10930681, 0.29992266, 0.52509575,\n",
       "        0.07455773, 0.21859035, 0.36095495, 0.07875481, 0.20179195,\n",
       "        0.32860274, 0.07047024, 0.1975142 , 0.34123797, 0.08472362,\n",
       "        0.23687983, 0.3911078 , 0.08078084, 0.23367562, 0.39153032,\n",
       "        0.08288922, 0.23775768, 0.39359927, 0.09736238, 0.27655735,\n",
       "        0.44502883, 0.0929132 , 0.25877075, 0.43297162, 0.09265914,\n",
       "        0.25861044, 0.42097845, 0.1031621 , 0.38522897, 0.49462757,\n",
       "        0.10417914, 0.29307399, 0.47440863, 0.10240946, 0.27630067,\n",
       "        0.46170774, 0.11171794, 0.32521858, 0.54246035, 0.11046147,\n",
       "        0.32080326, 0.50494595, 0.1062767 , 0.29233098, 0.49686322,\n",
       "        0.07193274, 0.20662222, 0.3370574 , 0.07129407, 0.20864587,\n",
       "        0.34225578, 0.07858038, 0.20716491, 0.33418446, 0.084831  ,\n",
       "        0.24052358, 0.39163437, 0.08356204, 0.23540015, 0.40499802,\n",
       "        0.0854918 , 0.23902011, 0.39894762, 0.09926114, 0.26426859,\n",
       "        0.44937396, 0.09573326, 0.26463623, 0.43781667, 0.09379125,\n",
       "        0.29043999, 0.4803772 , 0.11541457, 0.32617497, 0.50355978,\n",
       "        0.10390215, 0.28633432, 0.51154475, 0.10481834, 0.28678412,\n",
       "        0.46361742, 0.1074224 , 0.32272367, 0.52353649, 0.10782275,\n",
       "        0.30755138, 0.51214886, 0.10655632, 0.30328503, 0.50032687,\n",
       "        0.06965356, 0.18716879, 0.30511546, 0.06848722, 0.19008555,\n",
       "        0.31967864, 0.06618967, 0.18779593, 0.30918345, 0.07861161,\n",
       "        0.21413856, 0.35451212, 0.0769495 , 0.2199472 , 0.36420321,\n",
       "        0.07771816, 0.20932069, 0.33838725, 0.08108716, 0.2338531 ,\n",
       "        0.38128657, 0.08298597, 0.225213  , 0.37805586, 0.08272052,\n",
       "        0.23113174, 0.37497396, 0.08970571, 0.24734559, 0.41867814,\n",
       "        0.09169078, 0.25134692, 0.41068964, 0.08512626, 0.24134431,\n",
       "        0.39082842, 0.09366546, 0.2653842 , 0.43383751, 0.1016274 ,\n",
       "        0.26007133, 0.41424212, 0.08734517, 0.23802776, 0.40500007,\n",
       "        0.06859303, 0.18733473, 0.30604281, 0.06732116, 0.18620744,\n",
       "        0.3233326 , 0.07499151, 0.18574085, 0.30570521, 0.07826695,\n",
       "        0.2089684 , 0.35309362, 0.07542963, 0.21792955, 0.3534822 ,\n",
       "        0.07557287, 0.20816092, 0.34734149, 0.08173261, 0.24229989,\n",
       "        0.38668699, 0.08104477, 0.2335875 , 0.3829926 , 0.07979918,\n",
       "        0.22614417, 0.36789517, 0.08961124, 0.25805988, 0.42256484,\n",
       "        0.08914671, 0.24966683, 0.41194377, 0.08719978, 0.24258242,\n",
       "        0.39222083, 0.09354801, 0.26875896, 0.4362689 , 0.09306359,\n",
       "        0.25425496, 0.42430077, 0.09223728, 0.24875784, 0.41753068,\n",
       "        0.0692915 , 0.19207721, 0.30162802, 0.06358118, 0.17850885,\n",
       "        0.29609861, 0.06489806, 0.18620915, 0.30570717, 0.07459197,\n",
       "        0.22065978, 0.35027704, 0.0740582 , 0.21655264, 0.34943857,\n",
       "        0.07734513, 0.20472956, 0.34294958, 0.08407116, 0.23849587,\n",
       "        0.38315864, 0.08148503, 0.23489194, 0.37948618, 0.08081508,\n",
       "        0.22317348, 0.37627544, 0.09151063, 0.24813643, 0.44398336,\n",
       "        0.13313141, 0.2361197 , 0.39124284, 0.0858222 , 0.23774252,\n",
       "        0.39718819, 0.09991083, 0.26352196, 0.43599997, 0.09227939,\n",
       "        0.26689606, 0.44733095, 0.09508896, 0.26009178, 0.43317204,\n",
       "        0.07120099, 0.19616342, 0.31121607, 0.06665902, 0.19102759,\n",
       "        0.31268697, 0.06757059, 0.19749942, 0.31429143, 0.07441983,\n",
       "        0.21121211, 0.36488385, 0.0775538 , 0.21744618, 0.35490365,\n",
       "        0.07545133, 0.20848861, 0.35123596, 0.08476291, 0.23283367,\n",
       "        0.37245646, 0.07852936, 0.23571563, 0.37671542, 0.08016119,\n",
       "        0.22681761, 0.37256336, 0.0903368 , 0.25131226, 0.41701775,\n",
       "        0.08950939, 0.24517059, 0.41636157, 0.0903862 , 0.25664086,\n",
       "        0.38503933, 0.09076481, 0.26473742, 0.4451777 , 0.09265289,\n",
       "        0.25707545, 0.42729287, 0.09137144, 0.25334635, 0.4218122 ,\n",
       "        0.06845517, 0.18997045, 0.314083  , 0.06739335, 0.18919945,\n",
       "        0.30885453, 0.06724896, 0.18926272, 0.31313024, 0.08017912,\n",
       "        0.21461515, 0.3452282 , 0.07342172, 0.20474849, 0.35242338,\n",
       "        0.07747688, 0.21987739, 0.35347624, 0.08626518, 0.23230772,\n",
       "        0.38965182, 0.08124356, 0.23252563, 0.38260717, 0.07899313,\n",
       "        0.22710185, 0.38155513, 0.09453435, 0.25179219, 0.40552111,\n",
       "        0.08410101, 0.37810717, 0.43681121, 0.09322238, 0.25560274,\n",
       "        0.40428534, 0.09717064, 0.26880088, 0.44591494, 0.09305258,\n",
       "        0.27026987, 0.43238716, 0.08487   , 0.2510942 , 0.36980944]),\n",
       " 'std_fit_time': array([0.00338782, 0.0054467 , 0.00466835, 0.00335597, 0.00336676,\n",
       "        0.00371575, 0.00278192, 0.00334222, 0.00677751, 0.01193684,\n",
       "        0.00592124, 0.00400267, 0.0023436 , 0.00583735, 0.00536606,\n",
       "        0.0030524 , 0.00306398, 0.00939134, 0.00199034, 0.00469557,\n",
       "        0.00574662, 0.00166642, 0.00085283, 0.01257083, 0.0073316 ,\n",
       "        0.00199949, 0.00718705, 0.00288074, 0.00236083, 0.00853654,\n",
       "        0.00448319, 0.00596346, 0.00788172, 0.00289456, 0.00541872,\n",
       "        0.00544822, 0.00246017, 0.00366125, 0.01384329, 0.00693383,\n",
       "        0.01378598, 0.01055882, 0.00298465, 0.00583647, 0.00601254,\n",
       "        0.00275501, 0.00551904, 0.00468059, 0.0020639 , 0.00275048,\n",
       "        0.00521477, 0.00078216, 0.00757017, 0.00621034, 0.00340108,\n",
       "        0.02931683, 0.01006493, 0.00458393, 0.0036682 , 0.02296919,\n",
       "        0.00499572, 0.01152777, 0.01109358, 0.00427988, 0.00506716,\n",
       "        0.00273332, 0.00192058, 0.00272095, 0.00515328, 0.00268748,\n",
       "        0.00743878, 0.00681133, 0.00434205, 0.00771008, 0.00403714,\n",
       "        0.00347377, 0.00440512, 0.00284916, 0.00102891, 0.00377069,\n",
       "        0.00749045, 0.00359312, 0.0117989 , 0.00941992, 0.00194169,\n",
       "        0.0029298 , 0.0052883 , 0.00280503, 0.00592182, 0.04477563,\n",
       "        0.02101753, 0.02386912, 0.00749809, 0.0013027 , 0.00385232,\n",
       "        0.01094501, 0.00197445, 0.12218661, 0.07635425, 0.00908391,\n",
       "        0.0180699 , 0.01169207, 0.00429252, 0.00560846, 0.00665104,\n",
       "        0.00143505, 0.01568651, 0.0119362 , 0.00161123, 0.00428289,\n",
       "        0.00578162, 0.00184624, 0.00527518, 0.00879884, 0.00311206,\n",
       "        0.00436917, 0.01147449, 0.00299718, 0.00759513, 0.01237829,\n",
       "        0.00585748, 0.00728438, 0.0124763 , 0.00233401, 0.00455002,\n",
       "        0.00786221, 0.00268563, 0.00128318, 0.01043542, 0.00429288,\n",
       "        0.00379217, 0.13623715, 0.06384288, 0.04804167, 0.0190954 ,\n",
       "        0.00461964, 0.00435115, 0.00452474, 0.00216334, 0.00294729,\n",
       "        0.00165243, 0.0018094 , 0.00534696, 0.00579233, 0.00226489,\n",
       "        0.00189193, 0.0045104 , 0.00320347, 0.00303089, 0.00377209,\n",
       "        0.00400653, 0.00615856, 0.00731412, 0.00333257, 0.00542418,\n",
       "        0.10292575, 0.01329848, 0.12038164, 0.01082878, 0.00600275,\n",
       "        0.08560327, 0.04372175, 0.0518153 , 0.02462122, 0.02053487,\n",
       "        0.00166399, 0.00502165, 0.00652525, 0.00490513, 0.00394189,\n",
       "        0.00710489, 0.00270169, 0.01293674, 0.00657249, 0.00213397,\n",
       "        0.00592004, 0.00664077, 0.00369336, 0.01011851, 0.0102247 ,\n",
       "        0.00497438, 0.00325877, 0.00941241, 0.00515488, 0.00390625,\n",
       "        0.007716  , 0.00390701, 0.00848562, 0.01062239, 0.00581572,\n",
       "        0.00489282, 0.0041953 , 0.00318548, 0.00497849, 0.00559814,\n",
       "        0.00351664, 0.00362909, 0.00599342, 0.00287696, 0.0010147 ,\n",
       "        0.00580737, 0.00357231, 0.00536005, 0.01170506, 0.00153307,\n",
       "        0.00648384, 0.01103119, 0.00403575, 0.004061  , 0.00861083,\n",
       "        0.00501313, 0.00338161, 0.01016452, 0.0039503 , 0.00415189,\n",
       "        0.01310883, 0.00282047, 0.00598766, 0.00699297, 0.00233033,\n",
       "        0.00313131, 0.01097104, 0.00553486, 0.08535157, 0.09880521,\n",
       "        0.00872677, 0.00892695, 0.01152101, 0.00224466, 0.00133605,\n",
       "        0.00706941, 0.00460721, 0.01806129, 0.0101867 , 0.00082658,\n",
       "        0.00204669, 0.0099251 , 0.00234293, 0.00665089, 0.01269698,\n",
       "        0.00468252, 0.00884421, 0.01884394, 0.00451109, 0.00536352,\n",
       "        0.01207651, 0.00425568, 0.00436083, 0.01479989, 0.00829745,\n",
       "        0.00977187, 0.00488114, 0.00525401, 0.10569159, 0.05998987,\n",
       "        0.00669291, 0.00606683, 0.06449771, 0.00492507, 0.01071522,\n",
       "        0.01428559, 0.00313748, 0.01440591, 0.00638044, 0.00376778,\n",
       "        0.00322446, 0.00728862, 0.00364757, 0.04294533, 0.08805205,\n",
       "        0.00554835, 0.01847172, 0.05397911, 0.03993246, 0.05778939,\n",
       "        0.02902401, 0.01377895, 0.0203614 , 0.00982947, 0.00186042,\n",
       "        0.00344344, 0.00522332, 0.00362081, 0.00411362, 0.00567806,\n",
       "        0.00223809, 0.00363269, 0.00490082, 0.00362401, 0.00265281,\n",
       "        0.01031843, 0.00281831, 0.00327636, 0.00824089, 0.00357776,\n",
       "        0.00464427, 0.00621917, 0.00610888, 0.00545492, 0.00655104,\n",
       "        0.00162941, 0.00399631, 0.00610859, 0.00386277, 0.00378887,\n",
       "        0.00471948, 0.00241028, 0.00173426, 0.01616386, 0.0010125 ,\n",
       "        0.00623876, 0.01641972, 0.00366182, 0.00446163, 0.01648667,\n",
       "        0.00374401, 0.00592341, 0.00643006, 0.00385825, 0.00503837,\n",
       "        0.01194778, 0.0047817 , 0.00707116, 0.00618058, 0.00326219,\n",
       "        0.00687527, 0.01065267, 0.00148453, 0.0037775 , 0.00765332,\n",
       "        0.00216995, 0.00484859, 0.00716865, 0.00286336, 0.00669632,\n",
       "        0.00503675, 0.00374362, 0.00592953, 0.00849178, 0.00168785,\n",
       "        0.00613239, 0.00877253, 0.00318266, 0.00911836, 0.00866827,\n",
       "        0.00372464, 0.00371978, 0.00871139, 0.00315233, 0.00644333,\n",
       "        0.00898482, 0.00240688, 0.00179938, 0.00503074, 0.00255045,\n",
       "        0.00388896, 0.00479038, 0.00510441, 0.00284133, 0.02345206,\n",
       "        0.00248914, 0.0092368 , 0.01154435, 0.00161638, 0.00527173,\n",
       "        0.00927234, 0.00130829, 0.00157522, 0.01161264, 0.00361318,\n",
       "        0.00622493, 0.0101368 , 0.00100771, 0.00585818, 0.00383482,\n",
       "        0.00194302, 0.00387239, 0.00728781, 0.00240795, 0.00829025,\n",
       "        0.00353658, 0.00315671, 0.0030114 , 0.00532068, 0.00451472,\n",
       "        0.00713392, 0.00862326, 0.00281251, 0.05381457, 0.00719319,\n",
       "        0.00172956, 0.00424693, 0.00708421, 0.00461257, 0.00102161,\n",
       "        0.00691353, 0.00232587, 0.00838265, 0.00875492, 0.0051522 ,\n",
       "        0.00356523, 0.00702735, 0.00247808, 0.00332875, 0.00488349,\n",
       "        0.00167122, 0.00532527, 0.0071584 , 0.00083232, 0.00608947,\n",
       "        0.00718466, 0.00815164, 0.00441538, 0.0079491 , 0.00189234,\n",
       "        0.00366558, 0.00544431, 0.00437547, 0.00345283, 0.00703003,\n",
       "        0.00643736, 0.00382636, 0.0011613 , 0.00554725, 0.00493222,\n",
       "        0.01397477, 0.00290391, 0.00212705, 0.00745735, 0.00383577,\n",
       "        0.01827983, 0.0696515 , 0.01391465, 0.00955509, 0.01133924,\n",
       "        0.00449661, 0.0028601 , 0.00708439, 0.00423731, 0.00438699,\n",
       "        0.00654187, 0.00303529, 0.01145789, 0.00645945, 0.00282759,\n",
       "        0.00498161, 0.00697644, 0.00417985, 0.00534697, 0.01147712,\n",
       "        0.00418151, 0.00275429, 0.0035616 , 0.00369142, 0.00249492,\n",
       "        0.00392964, 0.00154035, 0.00192054, 0.00683529, 0.00189824,\n",
       "        0.0033744 , 0.00412713, 0.00167617, 0.01388319, 0.016774  ,\n",
       "        0.00288476, 0.00322195, 0.00511095, 0.00143114, 0.00400955,\n",
       "        0.00446286, 0.00250789, 0.00357225, 0.00644398, 0.00532307,\n",
       "        0.00887639, 0.00847252, 0.00251204, 0.0030067 , 0.01003738,\n",
       "        0.00387486, 0.00470858, 0.00297965, 0.00227961, 0.00467195,\n",
       "        0.00605928, 0.0020246 , 0.00369883, 0.00780412, 0.00485518,\n",
       "        0.01276434, 0.00970158, 0.00235202, 0.00637404, 0.00564147,\n",
       "        0.00418641, 0.00363095, 0.00510347, 0.00524125, 0.00378812,\n",
       "        0.01572616, 0.01155127, 0.00128182, 0.00419043, 0.00268157,\n",
       "        0.0032235 , 0.0045486 , 0.00175728, 0.00462974, 0.00526686,\n",
       "        0.00272045, 0.00176132, 0.00400074, 0.00180846, 0.00686982,\n",
       "        0.00935523, 0.00408662, 0.00909259, 0.00388523, 0.00149888,\n",
       "        0.00298592, 0.00404619, 0.00185348, 0.00408445, 0.00717929,\n",
       "        0.00377139, 0.00520889, 0.00607322, 0.00125927, 0.00480159,\n",
       "        0.00416741, 0.00125315, 0.01125851, 0.00827976, 0.00333121,\n",
       "        0.00346708, 0.0064319 , 0.00461233, 0.00730463, 0.00351787,\n",
       "        0.000893  , 0.00244739, 0.00506769, 0.0008498 , 0.00411425,\n",
       "        0.00379254, 0.00126102, 0.0018937 , 0.00381462, 0.00115603,\n",
       "        0.01304611, 0.00657879, 0.00260022, 0.00606861, 0.00574366,\n",
       "        0.00276126, 0.00283315, 0.00658677, 0.00358478, 0.00346796,\n",
       "        0.00982202, 0.00209292, 0.00427608, 0.01437294, 0.00223896,\n",
       "        0.005582  , 0.01208749, 0.00274782, 0.00515051, 0.06177809,\n",
       "        0.02397601, 0.00422162, 0.01293533, 0.00535576, 0.00241077,\n",
       "        0.0071362 , 0.00559744, 0.0015096 , 0.00872436, 0.00435357,\n",
       "        0.01550642, 0.01067437, 0.00292517, 0.00205591, 0.0104153 ,\n",
       "        0.00326045, 0.00440933, 0.00844961, 0.00186277, 0.0079912 ,\n",
       "        0.0015614 , 0.00433639, 0.00672476, 0.00769939, 0.00170768,\n",
       "        0.00301707, 0.00692068, 0.00248013, 0.00586796, 0.00565671,\n",
       "        0.00293979, 0.00372613, 0.00577809, 0.00127519, 0.0060405 ,\n",
       "        0.00459123, 0.00332767, 0.00743089, 0.00530683, 0.00194667,\n",
       "        0.00234691, 0.00628946, 0.00279175, 0.00600977, 0.0081459 ,\n",
       "        0.0027773 , 0.00411707, 0.02306495, 0.00716867, 0.02001997,\n",
       "        0.00865658, 0.00309312, 0.00661028, 0.00781856, 0.00155803,\n",
       "        0.00269188, 0.01096236, 0.00421032, 0.00481965, 0.00678453,\n",
       "        0.00215202, 0.00316546, 0.00680979, 0.00065115, 0.00266184,\n",
       "        0.00515609, 0.00484079, 0.0027373 , 0.00413262, 0.00741667,\n",
       "        0.01328875, 0.00780319, 0.00263073, 0.00455463, 0.01141585,\n",
       "        0.00241881, 0.00909212, 0.00789492, 0.00269684, 0.00613219,\n",
       "        0.00641366, 0.00164762, 0.00406862, 0.01131958, 0.00376895,\n",
       "        0.00607672, 0.00713584, 0.00325877, 0.00505069, 0.00497645,\n",
       "        0.00274549, 0.04717273, 0.01605266, 0.00732531, 0.01671705,\n",
       "        0.01374288, 0.00205281, 0.00455496, 0.00708133, 0.00222196,\n",
       "        0.00392052, 0.00627874, 0.0013263 , 0.00313291, 0.05174897]),\n",
       " 'mean_score_time': array([0.00580544, 0.00552778, 0.00688801, 0.00412984, 0.00508857,\n",
       "        0.00662603, 0.0050817 , 0.00501266, 0.00703621, 0.0060739 ,\n",
       "        0.00558486, 0.0086391 , 0.00420785, 0.00564222, 0.00747242,\n",
       "        0.00413823, 0.00557566, 0.00767965, 0.00516286, 0.00611882,\n",
       "        0.00814934, 0.0042726 , 0.00602221, 0.01001406, 0.00619602,\n",
       "        0.00580964, 0.00802083, 0.00449753, 0.00681481, 0.00951896,\n",
       "        0.00533772, 0.00665936, 0.00926619, 0.00442142, 0.00650916,\n",
       "        0.00822692, 0.00455704, 0.00700612, 0.01075625, 0.00455775,\n",
       "        0.00686193, 0.01096783, 0.00661178, 0.00691562, 0.00846181,\n",
       "        0.00394301, 0.00496111, 0.00646315, 0.00399389, 0.00499449,\n",
       "        0.0069006 , 0.00459175, 0.00671253, 0.00673175, 0.00445442,\n",
       "        0.00552039, 0.00987606, 0.00460882, 0.00565314, 0.00804834,\n",
       "        0.00463762, 0.00553255, 0.00759683, 0.00559106, 0.00716987,\n",
       "        0.00926194, 0.00421844, 0.00632019, 0.00818791, 0.0042006 ,\n",
       "        0.00596805, 0.00788794, 0.00474958, 0.00692611, 0.00920348,\n",
       "        0.00460472, 0.00627408, 0.0095439 , 0.004107  , 0.00623412,\n",
       "        0.00855908, 0.00460997, 0.00763655, 0.01041522, 0.00437393,\n",
       "        0.00694618, 0.00904632, 0.00456028, 0.00719509, 0.00892286,\n",
       "        0.009269  , 0.00542464, 0.00624952, 0.00360384, 0.00507259,\n",
       "        0.00614953, 0.00392599, 0.00845928, 0.0070744 , 0.00623927,\n",
       "        0.00560918, 0.00864353, 0.00505629, 0.00565324, 0.0075336 ,\n",
       "        0.00411396, 0.00703163, 0.00721164, 0.00415859, 0.00623817,\n",
       "        0.00908384, 0.00418735, 0.00669532, 0.00828919, 0.00423064,\n",
       "        0.006004  , 0.00803328, 0.00447459, 0.00739498, 0.00940638,\n",
       "        0.00534883, 0.00669599, 0.00897284, 0.00547662, 0.00673742,\n",
       "        0.01092944, 0.00500641, 0.00742888, 0.01013532, 0.00490875,\n",
       "        0.0069942 , 0.01428208, 0.00467315, 0.00912776, 0.01012421,\n",
       "        0.00461593, 0.00553617, 0.00653987, 0.0040772 , 0.0052402 ,\n",
       "        0.00676332, 0.00400481, 0.0051971 , 0.00682278, 0.00469055,\n",
       "        0.00606461, 0.0079329 , 0.0048121 , 0.00638785, 0.008394  ,\n",
       "        0.00429363, 0.00617456, 0.00822635, 0.00515423, 0.00645618,\n",
       "        0.01061616, 0.00407982, 0.00641718, 0.01078582, 0.00547304,\n",
       "        0.00853562, 0.01096435, 0.00508237, 0.00906944, 0.01181755,\n",
       "        0.00506244, 0.00733924, 0.00938258, 0.00521469, 0.0065351 ,\n",
       "        0.01094861, 0.00505133, 0.00782971, 0.01032991, 0.00591817,\n",
       "        0.00746102, 0.01013861, 0.00483265, 0.00714555, 0.00975418,\n",
       "        0.0047576 , 0.00555677, 0.00684614, 0.00451517, 0.00560384,\n",
       "        0.00718193, 0.00436902, 0.00582714, 0.00722122, 0.00460992,\n",
       "        0.00626163, 0.00818439, 0.00445738, 0.00638256, 0.00805001,\n",
       "        0.00457797, 0.00661635, 0.00843105, 0.00533094, 0.00693798,\n",
       "        0.00908136, 0.00619402, 0.00679502, 0.00955725, 0.00465961,\n",
       "        0.00698462, 0.00952835, 0.0049233 , 0.00759964, 0.01069722,\n",
       "        0.00499082, 0.00722609, 0.0107182 , 0.00482998, 0.00715399,\n",
       "        0.00959635, 0.00533848, 0.00773826, 0.01075578, 0.00644112,\n",
       "        0.00807161, 0.01078672, 0.0048213 , 0.01383934, 0.01011944,\n",
       "        0.00506315, 0.00562253, 0.006881  , 0.0044754 , 0.00562701,\n",
       "        0.00850973, 0.00518918, 0.00596938, 0.00715346, 0.00448866,\n",
       "        0.00617723, 0.00997453, 0.00468426, 0.00631466, 0.00952191,\n",
       "        0.00525637, 0.00668521, 0.00781903, 0.00497317, 0.00702057,\n",
       "        0.00996184, 0.00487685, 0.00656781, 0.00902233, 0.00505805,\n",
       "        0.00724335, 0.00896611, 0.00520778, 0.02058601, 0.01023283,\n",
       "        0.00472541, 0.00692539, 0.00979338, 0.00483437, 0.00782542,\n",
       "        0.00908537, 0.00538116, 0.00839038, 0.01105399, 0.00521832,\n",
       "        0.00730562, 0.01019583, 0.00473742, 0.0081665 , 0.01119552,\n",
       "        0.00436578, 0.00742249, 0.00872412, 0.00559402, 0.00528874,\n",
       "        0.00916033, 0.00569539, 0.00584617, 0.007376  , 0.00468712,\n",
       "        0.00716882, 0.00837445, 0.00452719, 0.00640554, 0.00884643,\n",
       "        0.00511174, 0.00612836, 0.00806513, 0.00490685, 0.00675735,\n",
       "        0.00922866, 0.00479383, 0.00701203, 0.00934539, 0.00495677,\n",
       "        0.00658145, 0.00854497, 0.00435982, 0.0075192 , 0.01133065,\n",
       "        0.0052567 , 0.0076364 , 0.00972128, 0.00504107, 0.00649409,\n",
       "        0.00941677, 0.00495086, 0.00845218, 0.01020155, 0.00484924,\n",
       "        0.0074862 , 0.01060367, 0.00515261, 0.0068965 , 0.00950398,\n",
       "        0.00476608, 0.00607853, 0.0075954 , 0.00465951, 0.00593767,\n",
       "        0.00792265, 0.0045752 , 0.00547423, 0.00706658, 0.00477853,\n",
       "        0.00652995, 0.0083034 , 0.0055759 , 0.00601478, 0.00807772,\n",
       "        0.0050252 , 0.00637765, 0.01027656, 0.00483599, 0.00697761,\n",
       "        0.00965486, 0.00474677, 0.00635962, 0.00901842, 0.00458789,\n",
       "        0.00655484, 0.00931144, 0.00464301, 0.00774016, 0.01027269,\n",
       "        0.00515943, 0.00699263, 0.01031184, 0.00513883, 0.00701914,\n",
       "        0.0090971 , 0.0047924 , 0.00711493, 0.01029773, 0.00500464,\n",
       "        0.00715618, 0.00982337, 0.00519919, 0.00771165, 0.00967145,\n",
       "        0.00435991, 0.0078723 , 0.00705309, 0.004422  , 0.00537801,\n",
       "        0.00674725, 0.00429053, 0.00536799, 0.00707808, 0.00443687,\n",
       "        0.00610051, 0.00802498, 0.00430732, 0.00607076, 0.00837879,\n",
       "        0.00457506, 0.00635214, 0.00807819, 0.00466342, 0.00691042,\n",
       "        0.00894318, 0.00444074, 0.00757308, 0.00890079, 0.00474172,\n",
       "        0.00643539, 0.01006217, 0.00470438, 0.00764141, 0.00972509,\n",
       "        0.00511146, 0.00712523, 0.00962758, 0.00496693, 0.00679026,\n",
       "        0.00934739, 0.0047668 , 0.00811272, 0.0106658 , 0.00496616,\n",
       "        0.00835838, 0.00981331, 0.00520053, 0.00680223, 0.00957623,\n",
       "        0.00456238, 0.00576525, 0.00678596, 0.00425029, 0.00502782,\n",
       "        0.00792713, 0.00419168, 0.005305  , 0.00718126, 0.00537472,\n",
       "        0.00611572, 0.00821905, 0.00590353, 0.00623627, 0.00874066,\n",
       "        0.00614538, 0.00608253, 0.00823221, 0.00530987, 0.00655179,\n",
       "        0.00956836, 0.00578218, 0.00667191, 0.00892482, 0.00448418,\n",
       "        0.006742  , 0.00883079, 0.00476255, 0.00734148, 0.01067238,\n",
       "        0.00500646, 0.00660801, 0.01169572, 0.0050365 , 0.0068222 ,\n",
       "        0.00943441, 0.00481076, 0.00747075, 0.01014438, 0.00469618,\n",
       "        0.00694742, 0.00976934, 0.00472398, 0.00704594, 0.00971336,\n",
       "        0.00422897, 0.00624356, 0.00676727, 0.0041142 , 0.00540166,\n",
       "        0.00719628, 0.00540838, 0.00536437, 0.00683379, 0.0045423 ,\n",
       "        0.00585723, 0.00811772, 0.00494971, 0.00721192, 0.00779734,\n",
       "        0.00457067, 0.00551653, 0.00778356, 0.0045372 , 0.00764213,\n",
       "        0.00890222, 0.00444403, 0.00625005, 0.00861659, 0.00447502,\n",
       "        0.00715885, 0.00821009, 0.00484452, 0.00678039, 0.00957804,\n",
       "        0.00488181, 0.00670276, 0.00890179, 0.00465813, 0.00659242,\n",
       "        0.00850558, 0.00473094, 0.00760083, 0.0112184 , 0.00655231,\n",
       "        0.00714192, 0.0092732 , 0.00456147, 0.00629001, 0.00865912,\n",
       "        0.00423121, 0.00537486, 0.00735459, 0.003864  , 0.00536623,\n",
       "        0.00717378, 0.00505562, 0.00538697, 0.00695205, 0.00431466,\n",
       "        0.00593457, 0.00809069, 0.00460377, 0.00587783, 0.00811481,\n",
       "        0.00458021, 0.00596609, 0.00809965, 0.00443158, 0.00669527,\n",
       "        0.00978346, 0.00442638, 0.00645022, 0.00889034, 0.00442805,\n",
       "        0.00615945, 0.00841665, 0.0046207 , 0.00853248, 0.00968418,\n",
       "        0.00571389, 0.00689592, 0.00893307, 0.00458741, 0.00671906,\n",
       "        0.0108664 , 0.00479679, 0.00714507, 0.01046834, 0.00548282,\n",
       "        0.00679107, 0.00940795, 0.00465455, 0.00667462, 0.00889435,\n",
       "        0.0044126 , 0.00551276, 0.00656219, 0.00454488, 0.00518751,\n",
       "        0.0071538 , 0.00433183, 0.0053679 , 0.00687475, 0.00431819,\n",
       "        0.00635843, 0.00978851, 0.00510645, 0.00592952, 0.00793867,\n",
       "        0.00460424, 0.00574079, 0.00870767, 0.00453863, 0.00631881,\n",
       "        0.0100801 , 0.00471835, 0.00624604, 0.00842776, 0.00461884,\n",
       "        0.00810022, 0.00851049, 0.00460935, 0.00665159, 0.0123713 ,\n",
       "        0.00730472, 0.00642557, 0.00873652, 0.00460334, 0.00657268,\n",
       "        0.00892344, 0.00473042, 0.00707574, 0.00941644, 0.00459943,\n",
       "        0.00734801, 0.00963821, 0.00472007, 0.00685663, 0.0093544 ,\n",
       "        0.00439739, 0.00530853, 0.00681529, 0.00439858, 0.00552683,\n",
       "        0.00680861, 0.00433059, 0.00594053, 0.00678864, 0.0043766 ,\n",
       "        0.00591846, 0.00849881, 0.00454478, 0.00746379, 0.00829148,\n",
       "        0.00482674, 0.00638657, 0.00907254, 0.0044281 , 0.00637774,\n",
       "        0.00852404, 0.00434427, 0.00632024, 0.00895982, 0.00442162,\n",
       "        0.00618792, 0.0092483 , 0.0050746 , 0.00725479, 0.01006188,\n",
       "        0.00521717, 0.00720778, 0.00917835, 0.00497427, 0.00643253,\n",
       "        0.00843196, 0.00470943, 0.00711331, 0.00996342, 0.00511785,\n",
       "        0.00683827, 0.00941668, 0.00514245, 0.00665054, 0.01000261,\n",
       "        0.00430284, 0.00602255, 0.00680265, 0.00431266, 0.00565162,\n",
       "        0.00700803, 0.00433502, 0.0063395 , 0.00710835, 0.00515914,\n",
       "        0.00554819, 0.00772128, 0.00419698, 0.00560141, 0.00803981,\n",
       "        0.00447884, 0.00586667, 0.00790806, 0.00526385, 0.00622129,\n",
       "        0.00879512, 0.00450168, 0.00611176, 0.00864143, 0.00441613,\n",
       "        0.00716329, 0.00915084, 0.00497856, 0.00655279, 0.00949273,\n",
       "        0.00451446, 0.00873938, 0.00999084, 0.00529785, 0.0062995 ,\n",
       "        0.00936618, 0.00488739, 0.00730395, 0.01033802, 0.00486279,\n",
       "        0.00711083, 0.01006646, 0.00458298, 0.00663457, 0.00776296]),\n",
       " 'std_score_time': array([1.28071752e-03, 5.33392172e-04, 6.93275012e-04, 1.67364979e-04,\n",
       "        1.81716379e-04, 3.51268675e-04, 2.36006831e-03, 9.49454029e-05,\n",
       "        8.11485547e-04, 3.54600985e-03, 7.71487451e-05, 7.12233915e-04,\n",
       "        2.99585786e-04, 2.24046971e-04, 2.93877785e-04, 1.97645350e-04,\n",
       "        1.91434234e-04, 7.63171524e-04, 1.48241743e-03, 7.05600419e-05,\n",
       "        5.16805255e-05, 2.49371273e-04, 6.79702894e-05, 3.39100876e-03,\n",
       "        3.94666134e-03, 7.62105426e-05, 5.57994455e-04, 1.41139449e-04,\n",
       "        2.64480239e-04, 6.09661260e-04, 1.83608053e-03, 3.08875612e-04,\n",
       "        2.85916483e-04, 2.44218862e-04, 7.12690228e-04, 1.36758560e-04,\n",
       "        1.10230127e-04, 1.27259735e-04, 1.72740701e-03, 4.46472911e-04,\n",
       "        4.46140603e-04, 2.49502852e-03, 2.99366426e-03, 1.29397009e-03,\n",
       "        1.99839751e-04, 2.02768934e-04, 5.12943818e-05, 2.31423993e-04,\n",
       "        1.89369116e-04, 6.24217177e-05, 6.00999384e-04, 9.46436218e-04,\n",
       "        1.51958557e-03, 3.79683874e-04, 5.43668880e-04, 4.02413272e-05,\n",
       "        4.21579049e-03, 7.73806815e-04, 1.77179496e-04, 4.38667546e-04,\n",
       "        1.07196379e-03, 1.21459457e-04, 6.28773352e-04, 2.14035557e-03,\n",
       "        1.52080574e-03, 1.55765089e-03, 8.26494571e-05, 6.88844798e-04,\n",
       "        3.93869992e-04, 8.29860518e-05, 1.89763780e-04, 1.09965162e-04,\n",
       "        5.57781117e-04, 4.47564584e-04, 2.49594051e-04, 3.45463095e-04,\n",
       "        4.05069520e-04, 1.58027679e-03, 3.71724247e-04, 2.01164673e-04,\n",
       "        1.78393480e-04, 2.44571940e-04, 1.35531826e-03, 1.49978402e-03,\n",
       "        3.26118272e-05, 2.97462951e-04, 1.59237411e-04, 2.02757440e-04,\n",
       "        1.69492156e-03, 3.14624745e-04, 5.85192677e-03, 7.20367407e-04,\n",
       "        5.80568773e-05, 5.01317221e-04, 7.18862658e-05, 6.65444953e-04,\n",
       "        6.42905590e-05, 3.85013352e-03, 5.03747087e-04, 2.79617979e-03,\n",
       "        3.20357380e-04, 1.75970488e-03, 1.13734901e-03, 8.23368958e-05,\n",
       "        1.98660135e-04, 8.48544284e-05, 1.83457131e-03, 4.65045569e-05,\n",
       "        1.21940286e-04, 2.18452181e-04, 1.62347681e-03, 4.57820789e-05,\n",
       "        6.05228583e-04, 3.29673866e-04, 1.36862283e-04, 2.25975871e-04,\n",
       "        2.45311702e-04, 1.07519473e-04, 1.30153678e-03, 4.43236526e-04,\n",
       "        1.42330189e-03, 2.35924888e-04, 1.27014365e-04, 1.64618351e-03,\n",
       "        3.10174101e-04, 2.50231720e-03, 4.22627652e-04, 5.09045319e-04,\n",
       "        4.08457652e-04, 4.09365475e-04, 2.39744856e-04, 5.09819544e-03,\n",
       "        3.00791797e-04, 2.40086302e-03, 9.46156037e-04, 1.04826075e-03,\n",
       "        4.84780281e-04, 1.54770382e-04, 1.54905525e-04, 1.75208751e-04,\n",
       "        4.06407378e-04, 1.35006958e-04, 1.18822558e-04, 8.84402533e-04,\n",
       "        4.09864508e-04, 5.60869122e-04, 2.18013109e-04, 1.05811030e-03,\n",
       "        8.36673180e-04, 9.74669650e-04, 1.47019606e-04, 2.89602682e-04,\n",
       "        6.04279579e-04, 7.27425776e-04, 1.38632513e-04, 2.81670686e-03,\n",
       "        6.16590347e-04, 3.62200578e-04, 2.10680597e-03, 2.11212585e-03,\n",
       "        3.02843402e-03, 2.46184953e-03, 4.55439574e-04, 2.87615614e-03,\n",
       "        2.56103937e-03, 8.45550993e-04, 9.17329715e-04, 3.90556548e-04,\n",
       "        6.86596299e-04, 9.66455406e-05, 2.65712144e-03, 5.67715688e-04,\n",
       "        4.87965019e-04, 4.51108124e-04, 1.88001691e-03, 3.92506484e-04,\n",
       "        4.43088844e-04, 1.73375565e-04, 5.42200346e-04, 2.82844955e-04,\n",
       "        3.38363977e-04, 3.20165293e-04, 1.92563553e-04, 2.38829878e-04,\n",
       "        2.95817544e-04, 6.25528724e-04, 3.04089154e-04, 4.06351813e-04,\n",
       "        4.36752305e-04, 2.73788320e-04, 5.73458755e-04, 2.92879089e-04,\n",
       "        4.54883420e-04, 4.01119760e-04, 5.26981292e-04, 3.94758234e-04,\n",
       "        5.69772519e-04, 6.33221570e-04, 9.31371987e-04, 3.09045590e-04,\n",
       "        4.10311739e-04, 1.55601673e-03, 2.68657333e-04, 4.82944831e-04,\n",
       "        2.16610458e-04, 2.68449012e-04, 4.34265299e-04, 6.77799155e-05,\n",
       "        4.49096273e-04, 7.99479257e-04, 4.25223369e-04, 1.96355937e-04,\n",
       "        5.59288188e-04, 8.79321336e-05, 1.77210806e-04, 2.53837712e-04,\n",
       "        4.74782221e-04, 1.47845097e-04, 2.32876597e-04, 2.38378299e-03,\n",
       "        6.43793234e-04, 4.15148966e-04, 9.77335890e-05, 9.30965153e-03,\n",
       "        1.94110188e-03, 1.19828653e-03, 2.47969242e-04, 1.79369699e-04,\n",
       "        1.81711273e-04, 2.05742475e-04, 1.50817609e-03, 8.67224056e-04,\n",
       "        5.43128295e-04, 1.87388898e-04, 1.54311194e-04, 4.92116739e-05,\n",
       "        2.43125370e-03, 1.55145358e-04, 1.61352253e-04, 1.34081251e-03,\n",
       "        5.03046510e-04, 5.26142462e-04, 9.55639959e-05, 3.16191076e-04,\n",
       "        3.15142742e-04, 5.64151118e-04, 2.39082414e-04, 7.95085890e-05,\n",
       "        6.85314465e-04, 7.38357188e-04, 1.08917080e-03, 3.94171968e-04,\n",
       "        1.69853720e-04, 1.28090743e-02, 6.35836928e-04, 2.18152786e-04,\n",
       "        2.34274876e-04, 3.39481689e-04, 1.99052723e-04, 8.46746655e-04,\n",
       "        2.19794280e-04, 5.05354652e-04, 1.52350778e-03, 1.32108744e-03,\n",
       "        2.76038778e-04, 2.31858458e-04, 4.77059946e-04, 1.78884264e-04,\n",
       "        2.16509537e-03, 2.05468252e-03, 1.50901189e-04, 1.74148851e-03,\n",
       "        2.47585550e-03, 1.12114828e-03, 4.86203837e-04, 4.07006049e-03,\n",
       "        2.58370436e-03, 2.13714063e-04, 3.56418899e-04, 2.28675083e-04,\n",
       "        1.95021316e-03, 1.76377482e-04, 1.04444276e-04, 2.91548915e-04,\n",
       "        5.69397730e-04, 6.63623156e-04, 1.12676368e-04, 3.41514741e-04,\n",
       "        2.90870174e-04, 1.47719780e-04, 2.44368897e-04, 2.53835132e-04,\n",
       "        5.50532374e-04, 3.17939077e-04, 3.97309505e-04, 3.31667739e-04,\n",
       "        1.77118298e-04, 5.74112089e-04, 4.69272825e-04, 2.05047623e-03,\n",
       "        6.37638112e-04, 5.31663651e-04, 3.23542533e-04, 4.65495883e-04,\n",
       "        8.73522640e-04, 2.41348404e-04, 1.14501361e-04, 6.18571846e-04,\n",
       "        2.90850443e-04, 1.88381120e-04, 4.27916880e-04, 7.56346168e-04,\n",
       "        2.70805499e-04, 1.89063197e-04, 1.90347493e-04, 2.41680227e-04,\n",
       "        4.74289847e-04, 5.59514748e-04, 6.12614312e-04, 4.24442170e-04,\n",
       "        7.32594211e-04, 3.85439730e-04, 2.49172971e-04, 1.59382676e-04,\n",
       "        3.70931641e-04, 7.72809257e-04, 3.68411622e-04, 1.65849562e-03,\n",
       "        2.79066901e-04, 2.91184061e-04, 1.18669956e-03, 5.37788215e-04,\n",
       "        2.74554498e-03, 2.57005656e-04, 3.10478461e-04, 4.56884294e-04,\n",
       "        1.35895963e-04, 6.05197093e-05, 8.04165687e-04, 2.49926135e-04,\n",
       "        3.79824129e-04, 8.77284843e-04, 5.57669975e-05, 6.51756800e-04,\n",
       "        2.23887177e-04, 2.24891560e-04, 8.59285697e-05, 6.98722311e-04,\n",
       "        2.78596875e-04, 2.50341851e-04, 3.99371004e-04, 1.10513984e-04,\n",
       "        7.01792309e-04, 2.41704583e-04, 5.73763056e-04, 2.30208881e-04,\n",
       "        1.41426187e-04, 3.65051935e-04, 7.02622182e-04, 6.42744139e-04,\n",
       "        1.75977747e-04, 4.57589925e-03, 1.71377119e-04, 2.28402779e-04,\n",
       "        1.56974619e-04, 9.42085908e-05, 1.98951034e-04, 8.94666473e-05,\n",
       "        2.79684672e-04, 2.69204481e-04, 3.73326089e-04, 1.52580023e-04,\n",
       "        7.03420311e-05, 9.27134537e-05, 3.92987837e-04, 2.60559130e-04,\n",
       "        4.62363384e-04, 7.48051885e-05, 1.51949860e-04, 2.07500805e-04,\n",
       "        3.34817946e-04, 4.72122729e-05, 2.26819670e-03, 2.01738898e-04,\n",
       "        2.51613888e-04, 1.75828295e-04, 2.55642265e-03, 1.28988162e-04,\n",
       "        6.94369029e-04, 9.36196034e-04, 4.60546474e-04, 2.65075985e-04,\n",
       "        3.60948280e-04, 4.02042795e-04, 2.75400833e-04, 2.83630558e-04,\n",
       "        3.69807430e-05, 4.42034280e-04, 3.22311219e-04, 2.97052772e-04,\n",
       "        1.84939903e-03, 1.31847759e-04, 7.16529881e-04, 4.58812010e-05,\n",
       "        6.57454898e-04, 7.27512688e-04, 4.98625937e-04, 1.67606097e-04,\n",
       "        1.41101586e-04, 5.96132795e-04, 1.79389236e-03, 8.66889954e-05,\n",
       "        6.37612413e-05, 3.61511304e-04, 1.29587228e-03, 1.86702628e-04,\n",
       "        4.77040472e-04, 1.94954442e-03, 3.48916916e-04, 1.22033683e-03,\n",
       "        3.41493283e-03, 4.40482438e-05, 3.06421234e-04, 8.22323112e-04,\n",
       "        2.16222460e-04, 8.55480236e-04, 9.43945415e-04, 4.26615602e-04,\n",
       "        2.83498182e-04, 8.85774276e-05, 4.63136126e-04, 3.79587297e-04,\n",
       "        7.96991817e-04, 1.73475862e-04, 1.41522487e-03, 5.85100365e-04,\n",
       "        2.66678401e-04, 3.05525344e-03, 3.09652800e-04, 2.74510253e-04,\n",
       "        3.61554725e-04, 3.55637268e-04, 1.61548362e-04, 1.66466891e-04,\n",
       "        2.25805701e-04, 9.37084625e-05, 2.22348231e-04, 1.84505753e-04,\n",
       "        3.68699495e-04, 5.81080409e-04, 3.53789187e-04, 1.38267080e-03,\n",
       "        2.30774308e-04, 1.17533289e-04, 2.06285200e-04, 5.89776914e-04,\n",
       "        1.08843077e-03, 7.07801119e-05, 1.50396435e-04, 4.11316908e-04,\n",
       "        9.23564768e-05, 5.36612005e-04, 3.69514913e-04, 2.00609721e-03,\n",
       "        3.69695099e-04, 2.85577229e-04, 4.05480826e-04, 2.49774959e-04,\n",
       "        2.33513374e-04, 2.33078748e-03, 7.71304254e-04, 4.78129067e-05,\n",
       "        2.02068859e-04, 2.33611892e-04, 1.08488743e-04, 1.59636459e-03,\n",
       "        2.07687660e-04, 4.09937701e-04, 9.13128840e-05, 7.68515062e-05,\n",
       "        4.44748101e-04, 3.22902209e-05, 1.61369148e-04, 2.05932471e-04,\n",
       "        1.71985757e-04, 3.51797839e-04, 1.27192323e-04, 8.29160964e-04,\n",
       "        2.35087373e-03, 2.08443509e-03, 2.48168276e-04, 5.39175393e-04,\n",
       "        1.90588681e-04, 1.91686604e-04, 1.12872502e-04, 2.13960983e-04,\n",
       "        1.53526415e-04, 1.20591424e-03, 6.06249972e-04, 1.79592257e-04,\n",
       "        3.76569960e-04, 9.23169034e-04, 1.97853039e-04, 1.92917495e-04,\n",
       "        5.91891109e-05, 1.68214511e-04, 2.92106349e-04, 9.38613219e-05,\n",
       "        1.11997629e-04, 4.08370842e-04, 2.36518268e-04, 5.78621342e-04,\n",
       "        6.25514024e-04, 1.60451885e-04, 7.08704269e-04, 9.35714231e-04,\n",
       "        7.66440050e-05, 2.48754863e-04, 5.09252137e-04, 1.64345896e-04,\n",
       "        9.64792059e-05, 4.21657180e-04, 1.62966283e-04, 3.33458974e-03,\n",
       "        4.45635171e-04, 1.65860835e-03, 4.61618037e-04, 4.32633489e-05,\n",
       "        7.18394387e-05, 3.24523662e-04, 3.19636753e-03, 2.27702270e-04,\n",
       "        3.09215776e-04, 1.14357428e-03, 1.52991096e-03, 2.12342306e-04,\n",
       "        3.07770014e-04, 1.43811636e-04, 3.50421155e-04, 2.90158040e-04,\n",
       "        3.05271633e-04, 2.99472049e-04, 1.60731501e-04, 1.12441539e-03,\n",
       "        1.56151538e-04, 8.35832394e-04, 6.10683526e-04, 1.19516796e-04,\n",
       "        2.28371618e-04, 5.37064297e-05, 4.87995353e-04, 3.40975542e-03,\n",
       "        4.32951440e-04, 1.34441985e-04, 2.41350147e-04, 1.68562954e-04,\n",
       "        2.14797013e-05, 1.46696469e-03, 1.80924020e-04, 4.63820658e-05,\n",
       "        3.21741914e-03, 5.88230025e-04, 9.04736655e-05, 2.35686991e-04,\n",
       "        3.08549140e-04, 4.03572494e-03, 2.82377135e-04, 1.11807994e-04,\n",
       "        8.91479428e-05, 6.12318919e-03, 4.52278738e-03, 2.30000435e-04,\n",
       "        1.40656503e-04, 1.53916756e-04, 2.60299565e-04, 4.25986275e-04,\n",
       "        8.71951734e-05, 3.91504506e-05, 4.52599116e-04, 2.85436332e-05,\n",
       "        4.65266692e-04, 7.56640960e-05, 7.58987263e-05, 1.33034354e-04,\n",
       "        4.08207795e-04, 1.10580069e-04, 5.54224693e-05, 1.44009132e-04,\n",
       "        3.06506089e-04, 2.63021207e-04, 1.04453854e-04, 3.39782643e-04,\n",
       "        9.28212070e-04, 2.68837627e-04, 1.22583454e-04, 1.80500872e-04,\n",
       "        8.89758149e-04, 2.29198237e-04, 2.53569542e-03, 9.97326239e-04,\n",
       "        7.70862410e-04, 9.13565110e-04, 1.63204582e-03, 2.88075951e-05,\n",
       "        3.14879604e-04, 3.28257193e-04, 1.45855703e-04, 1.37705833e-04,\n",
       "        9.96049997e-04, 1.90217903e-04, 1.58551546e-04, 1.40513023e-03,\n",
       "        1.74991247e-03, 9.32914704e-04, 9.77258981e-04, 6.24110453e-04,\n",
       "        7.59557180e-04, 2.41198187e-04, 5.45896824e-04, 4.98965282e-04,\n",
       "        7.18804773e-05, 3.47796322e-04, 4.64808867e-05, 4.46235606e-04,\n",
       "        1.20106242e-03, 7.94420437e-05, 1.66132119e-04, 8.81260321e-04,\n",
       "        1.56716259e-04, 1.73223096e-03, 2.22809662e-04, 1.08491985e-03,\n",
       "        2.06546384e-04, 1.47854478e-04, 3.36199739e-04, 2.69393220e-04,\n",
       "        1.21271286e-04, 1.10427462e-03, 4.40342337e-04, 1.75527569e-03,\n",
       "        4.95396917e-04, 2.47416994e-04, 7.32463783e-05, 7.77825039e-05,\n",
       "        2.42260008e-04, 2.99857810e-04, 1.69947640e-04, 1.46739645e-04,\n",
       "        1.66317992e-03, 6.25295167e-05, 2.65414287e-04, 1.92714055e-04,\n",
       "        1.78681006e-05, 5.94909851e-04, 1.20682822e-04, 1.91350742e-03,\n",
       "        1.91542295e-03, 9.66859162e-04, 8.92990860e-05, 4.95024945e-04,\n",
       "        1.42804888e-04, 2.97080986e-03, 9.13724050e-04, 1.17628855e-03,\n",
       "        8.27368122e-05, 5.39209276e-04, 4.47533746e-04, 2.64479053e-04,\n",
       "        6.33015532e-04, 5.51760204e-04, 2.75882791e-04, 8.40664632e-04,\n",
       "        2.65635945e-04, 1.45036557e-04, 8.92898000e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.73267327, 0.7254902 , 0.68      , 0.72      , 0.71428571,\n",
       "        0.71287129, 0.70707071, 0.71428571, 0.71428571, 0.70707071,\n",
       "        0.72      , 0.69902913, 0.70707071, 0.69387755, 0.72727273,\n",
       "        0.70707071, 0.70103093, 0.72164948, 0.69387755, 0.69387755,\n",
       "        0.7184466 , 0.71428571, 0.72      , 0.73076923, 0.70833333,\n",
       "        0.72164948, 0.74747475, 0.68686869, 0.70588235, 0.71153846,\n",
       "        0.66666667, 0.7       , 0.73076923, 0.71428571, 0.72727273,\n",
       "        0.73267327, 0.71287129, 0.71153846, 0.7047619 , 0.68041237,\n",
       "        0.71287129, 0.72380952, 0.71428571, 0.72727273, 0.74      ,\n",
       "        0.72      , 0.73267327, 0.74      , 0.7       , 0.72727273,\n",
       "        0.72      , 0.70103093, 0.70707071, 0.71428571, 0.69387755,\n",
       "        0.72727273, 0.69306931, 0.7       , 0.72164948, 0.71428571,\n",
       "        0.69387755, 0.68686869, 0.71428571, 0.67326733, 0.7       ,\n",
       "        0.72380952, 0.7       , 0.70833333, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.70707071, 0.69306931, 0.72      , 0.70588235,\n",
       "        0.70707071, 0.72164948, 0.73267327, 0.69387755, 0.70707071,\n",
       "        0.74      , 0.67326733, 0.73786408, 0.71153846, 0.70707071,\n",
       "        0.70707071, 0.7254902 , 0.69387755, 0.69387755, 0.72      ,\n",
       "        0.66666667, 0.72      , 0.74509804, 0.68      , 0.72      ,\n",
       "        0.71428571, 0.69387755, 0.72      , 0.72727273, 0.67346939,\n",
       "        0.71428571, 0.72      , 0.67346939, 0.70707071, 0.71428571,\n",
       "        0.70707071, 0.68686869, 0.69387755, 0.67326733, 0.69387755,\n",
       "        0.68686869, 0.69306931, 0.72      , 0.71428571, 0.69387755,\n",
       "        0.69387755, 0.69387755, 0.67326733, 0.70707071, 0.72      ,\n",
       "        0.68686869, 0.71428571, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.70707071, 0.66      , 0.72      , 0.71153846, 0.7       ,\n",
       "        0.69387755, 0.72      , 0.69387755, 0.7       , 0.70707071,\n",
       "        0.66666667, 0.73267327, 0.72      , 0.65306122, 0.70707071,\n",
       "        0.70707071, 0.68686869, 0.72      , 0.72      , 0.65306122,\n",
       "        0.68686869, 0.71428571, 0.65306122, 0.70707071, 0.70707071,\n",
       "        0.66666667, 0.70707071, 0.70707071, 0.65306122, 0.67346939,\n",
       "        0.70707071, 0.65306122, 0.7       , 0.69387755, 0.67346939,\n",
       "        0.68686869, 0.68686869, 0.66666667, 0.69306931, 0.70707071,\n",
       "        0.66666667, 0.68686869, 0.70707071, 0.68686869, 0.68686869,\n",
       "        0.7       , 0.66666667, 0.7       , 0.71287129, 0.65306122,\n",
       "        0.68686869, 0.7       , 0.68686869, 0.69387755, 0.7       ,\n",
       "        0.60215054, 0.66666667, 0.66666667, 0.60215054, 0.66666667,\n",
       "        0.68686869, 0.60869565, 0.68      , 0.68686869, 0.63157895,\n",
       "        0.65306122, 0.66666667, 0.61052632, 0.65306122, 0.68      ,\n",
       "        0.63157895, 0.66666667, 0.70707071, 0.625     , 0.65306122,\n",
       "        0.68      , 0.63917526, 0.65306122, 0.69306931, 0.65979381,\n",
       "        0.68041237, 0.70707071, 0.63917526, 0.66      , 0.68627451,\n",
       "        0.63917526, 0.66666667, 0.68686869, 0.65979381, 0.69387755,\n",
       "        0.70707071, 0.63917526, 0.66      , 0.68627451, 0.63157895,\n",
       "        0.67346939, 0.68686869, 0.64583333, 0.69387755, 0.69387755]),\n",
       " 'split2_test_score': array([0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.76190476, 0.74074074, 0.72897196, 0.75471698, 0.74285714,\n",
       "        0.74285714, 0.75471698, 0.74285714, 0.74285714, 0.74285714,\n",
       "        0.72897196, 0.72222222, 0.73584906, 0.75      , 0.73584906,\n",
       "        0.76190476, 0.74285714, 0.75471698, 0.74285714, 0.74074074,\n",
       "        0.72897196, 0.76190476, 0.74766355, 0.74285714, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.75471698, 0.73394495, 0.73394495,\n",
       "        0.76190476, 0.74285714, 0.73584906, 0.74285714, 0.73584906,\n",
       "        0.72727273, 0.75471698, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.73076923, 0.73584906, 0.72727273,\n",
       "        0.75247525, 0.75471698, 0.72897196, 0.73076923, 0.74074074,\n",
       "        0.75      , 0.75      , 0.73584906, 0.75      , 0.72380952,\n",
       "        0.73786408, 0.74074074, 0.73584906, 0.75471698, 0.74766355,\n",
       "        0.75      , 0.75      , 0.75471698, 0.73786408, 0.73394495,\n",
       "        0.73394495, 0.73584906, 0.76190476, 0.73584906, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.72380952, 0.73394495, 0.72222222,\n",
       "        0.73076923, 0.72897196, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.75471698, 0.71153846, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.74074074, 0.73584906, 0.72380952, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75925926, 0.75229358, 0.73267327, 0.74766355,\n",
       "        0.73584906, 0.74509804, 0.75      , 0.74285714, 0.75247525,\n",
       "        0.7254902 , 0.73584906, 0.73267327, 0.74285714, 0.75471698,\n",
       "        0.74509804, 0.75      , 0.76190476, 0.73267327, 0.74285714,\n",
       "        0.72897196, 0.7254902 , 0.75471698, 0.74766355, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.7184466 , 0.74545455, 0.73394495,\n",
       "        0.71698113, 0.75471698, 0.74285714, 0.73786408, 0.73584906,\n",
       "        0.74285714, 0.7047619 , 0.73873874, 0.73394495, 0.73076923,\n",
       "        0.74766355, 0.73584906, 0.73076923, 0.73584906, 0.74285714,\n",
       "        0.74      , 0.76923077, 0.74766355, 0.74      , 0.75471698,\n",
       "        0.75471698, 0.74      , 0.76190476, 0.75471698, 0.74509804,\n",
       "        0.75      , 0.76635514, 0.74509804, 0.75      , 0.74285714,\n",
       "        0.73267327, 0.75      , 0.76190476, 0.74509804, 0.73584906,\n",
       "        0.75925926, 0.74509804, 0.74285714, 0.75471698, 0.73786408,\n",
       "        0.74285714, 0.76190476, 0.73267327, 0.74285714, 0.74545455,\n",
       "        0.74509804, 0.73786408, 0.75471698, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.74509804, 0.75471698, 0.74545455, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.73076923, 0.74285714, 0.74285714,\n",
       "        0.70833333, 0.74      , 0.75247525, 0.70833333, 0.73267327,\n",
       "        0.74509804, 0.75510204, 0.74      , 0.76470588, 0.74      ,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.74509804, 0.74509804,\n",
       "        0.74747475, 0.73786408, 0.75      , 0.76      , 0.75      ,\n",
       "        0.75      , 0.75247525, 0.74509804, 0.73076923, 0.74      ,\n",
       "        0.73786408, 0.74285714, 0.76      , 0.75      , 0.72222222,\n",
       "        0.74      , 0.73786408, 0.73076923, 0.74509804, 0.73786408,\n",
       "        0.75      , 0.74      , 0.74285714, 0.72897196, 0.74      ,\n",
       "        0.73786408, 0.72380952, 0.74509804, 0.73076923, 0.75      ]),\n",
       " 'split3_test_score': array([0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.81132075, 0.79245283, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.80769231, 0.81553398, 0.82242991,\n",
       "        0.8       , 0.79245283, 0.81132075, 0.8       , 0.79245283,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80769231, 0.78095238,\n",
       "        0.78095238, 0.82242991, 0.79245283, 0.79245283, 0.81904762,\n",
       "        0.81904762, 0.82352941, 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.78504673, 0.78504673, 0.81904762, 0.80769231,\n",
       "        0.80769231, 0.81904762, 0.78504673, 0.79245283, 0.81904762,\n",
       "        0.78504673, 0.76635514, 0.81904762, 0.7961165 , 0.80769231,\n",
       "        0.77669903, 0.79245283, 0.79245283, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.81132075, 0.7961165 , 0.80769231,\n",
       "        0.8       , 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.7961165 , 0.81904762, 0.82692308, 0.82692308, 0.79245283,\n",
       "        0.79245283, 0.82692308, 0.79245283, 0.78504673, 0.80769231,\n",
       "        0.81904762, 0.81904762, 0.81553398, 0.78846154, 0.77358491,\n",
       "        0.82692308, 0.77358491, 0.78504673, 0.81553398, 0.80769231,\n",
       "        0.81132075, 0.80769231, 0.79245283, 0.77777778, 0.80769231,\n",
       "        0.78504673, 0.78504673, 0.82692308, 0.81132075, 0.8       ,\n",
       "        0.77669903, 0.81132075, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.7961165 , 0.7961165 ,\n",
       "        0.8       , 0.78095238, 0.78431373, 0.8       , 0.78846154,\n",
       "        0.78431373, 0.80769231, 0.80769231, 0.80392157, 0.78095238,\n",
       "        0.78504673, 0.7961165 , 0.79245283, 0.78504673, 0.78846154,\n",
       "        0.8       , 0.81904762, 0.82692308, 0.78846154, 0.77358491,\n",
       "        0.80769231, 0.78846154, 0.78504673, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.78095238, 0.78095238, 0.80769231,\n",
       "        0.78504673, 0.79245283, 0.81553398, 0.81904762, 0.81132075,\n",
       "        0.78      , 0.76923077, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.78095238, 0.77669903,\n",
       "        0.80373832, 0.78095238, 0.77669903, 0.79245283, 0.79245283,\n",
       "        0.77669903, 0.79245283, 0.8       , 0.7961165 , 0.79245283,\n",
       "        0.79245283, 0.7961165 , 0.79245283, 0.79245283, 0.77669903,\n",
       "        0.79245283, 0.8       , 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.7961165 , 0.8       ,\n",
       "        0.81132075, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.79245283, 0.7962963 , 0.7961165 , 0.81132075, 0.8       ,\n",
       "        0.77419355, 0.78787879, 0.76470588, 0.77419355, 0.78787879,\n",
       "        0.76470588, 0.75789474, 0.78787879, 0.76470588, 0.77083333,\n",
       "        0.78      , 0.77669903, 0.7628866 , 0.78      , 0.77669903,\n",
       "        0.7755102 , 0.78      , 0.77669903, 0.82828283, 0.76470588,\n",
       "        0.78846154, 0.78787879, 0.76470588, 0.78846154, 0.7755102 ,\n",
       "        0.76      , 0.78846154, 0.79166667, 0.77227723, 0.8       ,\n",
       "        0.7628866 , 0.76470588, 0.79245283, 0.7628866 , 0.76      ,\n",
       "        0.8       , 0.8125    , 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.8       , 0.7628866 , 0.78      , 0.80769231]),\n",
       " 'split4_test_score': array([0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.75510204, 0.82      , 0.8       , 0.73267327, 0.74      ,\n",
       "        0.7628866 , 0.73267327, 0.72727273, 0.74226804, 0.7755102 ,\n",
       "        0.80851064, 0.8       , 0.75247525, 0.7755102 , 0.75510204,\n",
       "        0.74      , 0.73684211, 0.72916667, 0.78      , 0.80412371,\n",
       "        0.78350515, 0.76      , 0.76      , 0.76767677, 0.75247525,\n",
       "        0.75789474, 0.75510204, 0.7755102 , 0.81632653, 0.77894737,\n",
       "        0.76      , 0.76      , 0.78      , 0.76470588, 0.7628866 ,\n",
       "        0.7755102 , 0.79591837, 0.79591837, 0.79166667, 0.76      ,\n",
       "        0.76767677, 0.76      , 0.74747475, 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.79207921, 0.80392157, 0.72727273, 0.76      ,\n",
       "        0.76      , 0.73469388, 0.72      , 0.72727273, 0.74226804,\n",
       "        0.79207921, 0.79166667, 0.72164948, 0.76      , 0.75510204,\n",
       "        0.74747475, 0.73267327, 0.74226804, 0.76470588, 0.79591837,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.76767677, 0.73469388,\n",
       "        0.75510204, 0.7755102 , 0.78431373, 0.78787879, 0.78350515,\n",
       "        0.79207921, 0.76767677, 0.76767677, 0.76767677, 0.76      ,\n",
       "        0.7755102 , 0.79207921, 0.79591837, 0.78350515, 0.79207921,\n",
       "        0.76767677, 0.77227723, 0.7755102 , 0.76767677, 0.7755102 ,\n",
       "        0.73684211, 0.78431373, 0.8       , 0.74226804, 0.77669903,\n",
       "        0.76470588, 0.73684211, 0.72      , 0.72727273, 0.72164948,\n",
       "        0.78      , 0.8125    , 0.72727273, 0.76470588, 0.74747475,\n",
       "        0.73469388, 0.7254902 , 0.72727273, 0.73469388, 0.78431373,\n",
       "        0.82474227, 0.72727273, 0.74509804, 0.75247525, 0.74226804,\n",
       "        0.74509804, 0.73469388, 0.76767677, 0.8       , 0.79591837,\n",
       "        0.74747475, 0.75247525, 0.7755102 , 0.7628866 , 0.74      ,\n",
       "        0.73469388, 0.76      , 0.78      , 0.80808081, 0.78      ,\n",
       "        0.75247525, 0.76767677, 0.74226804, 0.74      , 0.73469388,\n",
       "        0.74157303, 0.75510204, 0.77227723, 0.72527473, 0.74      ,\n",
       "        0.7961165 , 0.72340426, 0.73469388, 0.73267327, 0.73913043,\n",
       "        0.75247525, 0.77227723, 0.72340426, 0.73267327, 0.76470588,\n",
       "        0.71578947, 0.71287129, 0.7254902 , 0.73684211, 0.77669903,\n",
       "        0.78787879, 0.72916667, 0.76923077, 0.76470588, 0.72340426,\n",
       "        0.73267327, 0.75247525, 0.72916667, 0.76470588, 0.80808081,\n",
       "        0.73684211, 0.75      , 0.76470588, 0.71578947, 0.73267327,\n",
       "        0.74      , 0.75      , 0.76470588, 0.81188119, 0.75      ,\n",
       "        0.77669903, 0.78      , 0.73684211, 0.73267327, 0.74747475,\n",
       "        0.71264368, 0.73333333, 0.72916667, 0.68235294, 0.73333333,\n",
       "        0.72164948, 0.6744186 , 0.72340426, 0.72164948, 0.69565217,\n",
       "        0.71578947, 0.73469388, 0.70967742, 0.72916667, 0.73469388,\n",
       "        0.65934066, 0.73469388, 0.72727273, 0.7032967 , 0.75      ,\n",
       "        0.73469388, 0.70967742, 0.74226804, 0.72727273, 0.68131868,\n",
       "        0.73469388, 0.74747475, 0.7173913 , 0.74226804, 0.74747475,\n",
       "        0.72527473, 0.73684211, 0.75247525, 0.68131868, 0.73469388,\n",
       "        0.75510204, 0.7173913 , 0.74226804, 0.78431373, 0.7173913 ,\n",
       "        0.73684211, 0.76470588, 0.68888889, 0.70833333, 0.76      ]),\n",
       " 'mean_test_score': array([0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77690605, 0.78603378, 0.76828496, 0.77347805, 0.77187816,\n",
       "        0.77669004, 0.76745829, 0.76985015, 0.77441755, 0.77268039,\n",
       "        0.77753426, 0.76655036, 0.77057392, 0.77530612, 0.7735639 ,\n",
       "        0.77032385, 0.77076142, 0.77344419, 0.77187569, 0.76704567,\n",
       "        0.76315953, 0.78259786, 0.77265073, 0.77056072, 0.77441784,\n",
       "        0.77916358, 0.78174424, 0.76802709, 0.77654212, 0.76029993,\n",
       "        0.77407283, 0.76845456, 0.77720679, 0.77617927, 0.77981706,\n",
       "        0.77786047, 0.78680788, 0.77063174, 0.76375919, 0.77301245,\n",
       "        0.77283779, 0.77146017, 0.77318925, 0.77109236, 0.77391298,\n",
       "        0.7646251 , 0.78137475, 0.78068832, 0.7549482 , 0.77434219,\n",
       "        0.77676923, 0.76111222, 0.76960539, 0.77061191, 0.75823537,\n",
       "        0.78231699, 0.76890487, 0.75783634, 0.77419637, 0.77277461,\n",
       "        0.76219964, 0.7685917 , 0.77851255, 0.76525796, 0.77145352,\n",
       "        0.7778565 , 0.76434561, 0.77166069, 0.76654815, 0.75953145,\n",
       "        0.77263213, 0.78197406, 0.77257608, 0.77304735, 0.76242354,\n",
       "        0.78224223, 0.76700408, 0.77427823, 0.7685618 , 0.7749526 ,\n",
       "        0.78392864, 0.76491546, 0.7796689 , 0.76211552, 0.78248889,\n",
       "        0.77040402, 0.77628166, 0.77489786, 0.7731734 , 0.78091922,\n",
       "        0.74732092, 0.77968463, 0.78328785, 0.7566251 , 0.77766011,\n",
       "        0.77320946, 0.76065374, 0.76662745, 0.77178074, 0.75344801,\n",
       "        0.77094547, 0.78073407, 0.75154582, 0.77380053, 0.77021857,\n",
       "        0.75894115, 0.76263769, 0.76738024, 0.75044967, 0.76902761,\n",
       "        0.77542296, 0.75309563, 0.77332736, 0.76912502, 0.75560104,\n",
       "        0.76335684, 0.76571429, 0.75804707, 0.77682481, 0.7733171 ,\n",
       "        0.75814001, 0.77061535, 0.76908725, 0.7635709 , 0.7649558 ,\n",
       "        0.76835292, 0.75282748, 0.77256567, 0.77553077, 0.77624133,\n",
       "        0.76610965, 0.77119573, 0.76119564, 0.77205626, 0.76841927,\n",
       "        0.74964794, 0.76995325, 0.77116892, 0.74568053, 0.76517546,\n",
       "        0.78036843, 0.74626107, 0.76874097, 0.76854231, 0.74279775,\n",
       "        0.76495308, 0.77764788, 0.74126867, 0.76506681, 0.7722911 ,\n",
       "        0.73190104, 0.76277599, 0.76919016, 0.74622357, 0.76203069,\n",
       "        0.78188134, 0.74795379, 0.7695356 , 0.77369967, 0.73738939,\n",
       "        0.75567627, 0.77279876, 0.75058119, 0.76415541, 0.77693914,\n",
       "        0.75000962, 0.76524358, 0.77466307, 0.74509245, 0.76047982,\n",
       "        0.7713846 , 0.75331055, 0.77037208, 0.782829  , 0.75082582,\n",
       "        0.77007257, 0.77734099, 0.74930298, 0.76477319, 0.7711433 ,\n",
       "        0.71057533, 0.74807576, 0.74497913, 0.70621506, 0.74832094,\n",
       "        0.75166442, 0.70753681, 0.74952191, 0.75681676, 0.72080438,\n",
       "        0.74041012, 0.75506823, 0.72166857, 0.74228993, 0.75699516,\n",
       "        0.70500313, 0.73642224, 0.75691438, 0.7365074 , 0.74755342,\n",
       "        0.75696772, 0.73871091, 0.74429194, 0.75591456, 0.72186217,\n",
       "        0.74177774, 0.76350946, 0.73743612, 0.74652522, 0.7551943 ,\n",
       "        0.73091412, 0.74448105, 0.7605132 , 0.72199334, 0.74198813,\n",
       "        0.76243455, 0.73181331, 0.75002141, 0.76160435, 0.72689311,\n",
       "        0.74842972, 0.76073338, 0.72238753, 0.7417797 , 0.76865061]),\n",
       " 'std_test_score': array([0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03468459, 0.04731971, 0.05669695, 0.05112019, 0.05627722,\n",
       "        0.05067905, 0.05101685, 0.05418655, 0.05325322, 0.04361997,\n",
       "        0.04450892, 0.04705291, 0.05086977, 0.05399647, 0.0474307 ,\n",
       "        0.04596631, 0.05849607, 0.04909612, 0.04950388, 0.04464262,\n",
       "        0.03334699, 0.0496852 , 0.04223026, 0.03221618, 0.05397928,\n",
       "        0.04964871, 0.03904538, 0.0470693 , 0.04964067, 0.03202918,\n",
       "        0.06598135, 0.05112595, 0.0444751 , 0.04686917, 0.051146  ,\n",
       "        0.04507871, 0.04858394, 0.04427648, 0.04180495, 0.06296765,\n",
       "        0.051426  , 0.05227151, 0.0540804 , 0.0423042 , 0.03825025,\n",
       "        0.03816129, 0.03518592, 0.04075258, 0.04914624, 0.04776831,\n",
       "        0.04941648, 0.04775401, 0.06341077, 0.05496931, 0.04958089,\n",
       "        0.04605519, 0.04897918, 0.04973751, 0.04176968, 0.04776757,\n",
       "        0.04459878, 0.06036219, 0.05310885, 0.05729076, 0.04813312,\n",
       "        0.04245504, 0.05436377, 0.0447545 , 0.04784204, 0.04860457,\n",
       "        0.05297003, 0.05570314, 0.0567386 , 0.04155167, 0.04368582,\n",
       "        0.05584914, 0.04322847, 0.03903681, 0.05219343, 0.05232356,\n",
       "        0.03616148, 0.06237612, 0.04188921, 0.03925779, 0.05371559,\n",
       "        0.0483467 , 0.04854556, 0.06032737, 0.05695128, 0.04513514,\n",
       "        0.04627783, 0.03721314, 0.0337867 , 0.05666147, 0.04448453,\n",
       "        0.05136025, 0.05026203, 0.04815016, 0.05320229, 0.05318021,\n",
       "        0.04538596, 0.04926359, 0.05646089, 0.05034035, 0.04468789,\n",
       "        0.04070753, 0.05619731, 0.05453729, 0.05031366, 0.04939743,\n",
       "        0.0605145 , 0.04866982, 0.0467345 , 0.04456927, 0.04236491,\n",
       "        0.04909074, 0.05427463, 0.05603018, 0.04673237, 0.04422357,\n",
       "        0.05431653, 0.04320362, 0.04282084, 0.04508954, 0.05897238,\n",
       "        0.05370971, 0.06357209, 0.0424579 , 0.0479248 , 0.05721302,\n",
       "        0.05175175, 0.04258794, 0.05030686, 0.06074379, 0.05184144,\n",
       "        0.05082291, 0.02993929, 0.03830383, 0.06216933, 0.04570842,\n",
       "        0.04792113, 0.04321683, 0.04409874, 0.04765355, 0.04998556,\n",
       "        0.04986715, 0.04487354, 0.05259948, 0.04792679, 0.04964042,\n",
       "        0.03950308, 0.0538643 , 0.05204479, 0.0531972 , 0.0538901 ,\n",
       "        0.05058668, 0.05720173, 0.04793628, 0.05494283, 0.03815762,\n",
       "        0.04771691, 0.05787219, 0.05644809, 0.04616912, 0.04741514,\n",
       "        0.05184756, 0.05616092, 0.0484463 , 0.04391026, 0.05364264,\n",
       "        0.05802794, 0.05058097, 0.04882764, 0.04886972, 0.05763106,\n",
       "        0.05447267, 0.05389363, 0.04186004, 0.05448764, 0.05680812,\n",
       "        0.0597256 , 0.05028869, 0.04754808, 0.06223935, 0.05289919,\n",
       "        0.05121671, 0.05800556, 0.04804207, 0.0533574 , 0.05196918,\n",
       "        0.0513209 , 0.05642011, 0.06001586, 0.05174424, 0.05536379,\n",
       "        0.05345758, 0.03865305, 0.04059513, 0.06837951, 0.05382093,\n",
       "        0.05107438, 0.05943977, 0.0527892 , 0.05202275, 0.04393994,\n",
       "        0.0376682 , 0.0427533 , 0.05519104, 0.04892661, 0.04920166,\n",
       "        0.0504471 , 0.04843146, 0.05236624, 0.0429899 , 0.0297756 ,\n",
       "        0.03491666, 0.056032  , 0.05486682, 0.04772114, 0.052434  ,\n",
       "        0.04923666, 0.05087227, 0.04760869, 0.0398153 , 0.04798134]),\n",
       " 'rank_test_score': array([301,  70, 199, 123, 185, 121, 393, 256,   3, 175, 318, 436, 482,\n",
       "        400, 192, 537, 115,  11,  15,  91, 369,  43, 177, 328,  87,  37,\n",
       "        117, 163, 102, 539, 310, 208, 243, 132, 126, 159, 100, 423, 596,\n",
       "        189,  73, 345,   9,  50,  78, 492, 113, 226, 194,  52, 107, 239,\n",
       "        151,  26, 434, 119, 250, 459, 332, 386, 447,  85,  21, 128,  83,\n",
       "         19, 235, 320,  39, 315,   5,  31, 149,  58, 170,  62,  64,  68,\n",
       "         95, 253,   1, 337, 136, 466, 153, 224, 201, 228, 109,  81, 564,\n",
       "        182,  24, 485, 231,  35, 417, 451, 259, 554, 216, 187, 560, 384,\n",
       "        335, 500, 281, 165, 534, 145, 134, 543, 271, 582, 407, 237, 284,\n",
       "        264,  76, 141, 471, 213, 279, 375, 233, 130, 343,  97, 143, 268,\n",
       "        168,  56, 356, 155,  45, 529, 274,  33, 603, 377, 218, 587, 352,\n",
       "        461, 572, 402,  13, 589, 241, 104, 591, 507, 382, 409, 247, 138,\n",
       "        566, 221, 277, 473, 339, 308, 324, 203,  47, 522,  66,  29, 457,\n",
       "        287, 354, 294, 349,  17, 455,  93, 147, 426, 261, 405, 641, 612,\n",
       "        505, 653, 619, 509, 661, 600, 580, 639, 527, 444, 637, 593, 494,\n",
       "        651, 548, 487, 520, 489, 421, 562, 430, 396, 644, 546, 313, 550,\n",
       "        440, 266, 432, 575, 359, 623, 515, 372, 389, 531, 428, 480, 503,\n",
       "        205, 608, 469, 211, 301,  70, 199, 123, 185, 121, 393, 256,   3,\n",
       "        175, 318, 436, 482, 400, 192, 537, 115,  11,  15,  91, 369,  43,\n",
       "        177, 328,  87,  37, 117, 163, 102, 539, 310, 208, 243, 132, 126,\n",
       "        159, 100, 423, 596, 189,  73, 345,   9,  50,  78, 492, 113, 226,\n",
       "        194,  52, 107, 239, 151,  26, 434, 119, 250, 459, 332, 386, 447,\n",
       "         85,  21, 128,  83,  19, 235, 320,  39, 315,   5,  31, 149,  58,\n",
       "        170,  62,  64,  68,  95, 253,   1, 337, 136, 466, 153, 224, 201,\n",
       "        228, 109,  81, 564, 182,  24, 485, 231,  35, 417, 451, 259, 554,\n",
       "        216, 187, 560, 384, 335, 500, 281, 165, 534, 145, 134, 543, 271,\n",
       "        582, 407, 237, 284, 264,  76, 141, 471, 213, 279, 375, 233, 130,\n",
       "        343,  97, 143, 268, 168,  56, 356, 155,  45, 529, 274,  33, 603,\n",
       "        377, 218, 587, 352, 461, 572, 402,  13, 589, 241, 104, 591, 507,\n",
       "        382, 409, 247, 138, 566, 221, 277, 473, 339, 308, 324, 203,  47,\n",
       "        522,  66,  29, 457, 287, 354, 294, 349,  17, 455,  93, 147, 426,\n",
       "        261, 405, 641, 612, 505, 653, 619, 509, 661, 600, 580, 639, 527,\n",
       "        444, 637, 593, 494, 651, 548, 487, 520, 489, 421, 562, 430, 396,\n",
       "        644, 546, 313, 550, 440, 266, 432, 575, 359, 623, 515, 372, 389,\n",
       "        531, 428, 480, 503, 205, 608, 469, 211, 179,   8, 420, 283, 322,\n",
       "        184, 438, 380, 246, 303, 167, 449, 364, 215, 276, 368, 358, 286,\n",
       "        323, 442, 502,  42, 304, 365, 245, 125,  72, 425, 191, 545, 258,\n",
       "        415, 173, 198, 106, 157,   7, 361, 496, 297, 298, 330, 292, 348,\n",
       "        263, 479,  75,  90, 586, 249, 181, 526, 381, 363, 556,  54, 399,\n",
       "        559, 255, 300, 517, 412, 140, 463, 331, 158, 484, 327, 450, 552,\n",
       "        305,  60, 306, 296, 514,  55, 443, 252, 413, 220,  23, 477, 112,\n",
       "        518,  49, 366, 196, 223, 293,  80, 628, 111,  28, 574, 161, 291,\n",
       "        536, 446, 326, 595, 351,  89, 606, 270, 371, 553, 512, 439, 611,\n",
       "        398, 210, 599, 289, 392, 579, 499, 454, 558, 180, 290, 557, 362,\n",
       "        395, 497, 475, 419, 602, 307, 207, 197, 453, 341, 525, 317, 416,\n",
       "        616, 379, 342, 632, 465,  99, 630, 404, 414, 643, 476, 162, 650,\n",
       "        468, 312, 663, 511, 391, 631, 519,  61, 626, 388, 273, 658, 578,\n",
       "        299, 610, 491, 174, 615, 464, 230, 633, 542, 334, 598, 367,  41,\n",
       "        607, 374, 172, 618, 478, 347, 672, 625, 634, 674, 622, 605, 673,\n",
       "        617, 571, 671, 655, 585, 670, 646, 568, 675, 660, 570, 659, 627,\n",
       "        569, 656, 636, 577, 669, 649, 498, 657, 629, 584, 665, 635, 541,\n",
       "        668, 647, 513, 664, 614, 524, 666, 621, 533, 667, 648, 411],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907208606102268"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2330e8d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
